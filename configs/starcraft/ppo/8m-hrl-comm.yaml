starcraft-8m-hrl-comm:
  run: PPO-hrl
  checkpoint_freq: 200
  checkpoint_at_end: true
#  restore: null
#  keep_checkpoints_num: 5
  local_dir: ray_results
  stop:
    timesteps_total: 1000000
  config:
    seed: 0
    framework: torch
    callbacks: PvEMetrics

    env: starcraft_pve_hrl_com
    env_config:
      map_name: 8m
      hrl_config:
        context_size: 5
        context_type: discrete
        high_level_interval: 10

    num_workers: 36
    num_cpus_for_driver: 1
    num_envs_per_worker: 1
    num_cpus_per_worker: 1
    num_gpus: 1
    num_gpus_per_worker: 0
    evaluation_num_workers: 4
    evaluation_interval: 10  # iterations
    evaluation_duration: 20
    evaluation_duration_unit: episodes
    evaluation_parallel_to_training: true
    disable_env_checking: true

    evaluation_config:
      env_config:
        map_name: 8m
        hrl_config:
          context_size: 5
          context_type: discrete
          high_level_interval: 10

    high_level_policy_config:
      model:
        custom_model: att_com_model
        custom_action_dist: hom_multi_action
        custom_model_config:
          encoder_hidden_layers: [256, 256]
          num_heads: 8
          head_dim: 64
          decoder_hidden_layers: [256]

    low_level_policy_config:
      model:
        custom_model: action_mask_model

    # For other configurations, see algorithms/ppo_hrl.py