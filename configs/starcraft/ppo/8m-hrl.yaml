starcraft-8m-hrl-ppo:
  run: PPO-hrl
  checkpoint_freq: 100
  checkpoint_at_end: true
  local_dir: ray_results
  stop:
    timesteps_total: 1000000
  config:
    seed: 0
    framework: torch
    callbacks: PvEMetrics
    env: starcraft_pve_hrl
    env_config:
      map_name: 8m
      hrl_config:
        context_size: 5
        context_type: discrete
        high_level_interval: 10

    num_workers: 36
    num_cpus_for_driver: 1
    num_envs_per_worker: 1
    num_cpus_per_worker: 1
    num_gpus: 1
    num_gpus_per_worker: 0
    evaluation_num_workers: 4
    evaluation_interval: 10  # iterations
    evaluation_duration: 20
    evaluation_duration_unit: episodes
    evaluation_parallel_to_training: true
    disable_env_checking: true

    evaluation_config:
      env_config:
        map_name: 8m
        hrl_config:
          context_size: 5
          context_type: discrete
          high_level_interval: 10

    low_level_policy_config:
      model:
        custom_model: action_mask_model

    # For other configurations, see algorithms/ppo_hrl.py